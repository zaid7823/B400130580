{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1OtyvJchN4acOpMeiwj4i1SLD8AYxyAlw","authorship_tag":"ABX9TyNx8RErpECM6nD7RhYu41DI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Importing the libraries\n","\n","import tensorflow as tf\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from mlxtend.plotting import plot_confusion_matrix\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline"],"metadata":{"id":"5jBO89JAovFd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setting the parameters\n","vocabSize = 10000\n","maxLen = 200"],"metadata":{"id":"0eO1ixCwuOqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the encoding type of the data\n","import chardet\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/LP 5/imdb.csv', 'rb') as f:\n","    result = chardet.detect(f.read(10000))\n","    print(result)\n","\n","# Reading the dataset & dropping irrelevant features\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/LP 5/imdb.csv', encoding = result['encoding'])\n","df = df.drop(df[['Unnamed: 0', 'type', 'file']], axis=1)  # removing unncessary columns\n","\n","# print(df[df['label'] == 'pos']) # to check only rows with positive reviews"],"metadata":{"id":"tfMcfmHztqrV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cleans the labels to only have POSITIVE & NEGATIVE labels, maps them to 1 and 0 resp.\n","\n","print(df['label'].unique())     # checks all unique label values\n","df = df[df['label'].isin(['pos', 'neg'])]   # removes unsup\n","df['label'] = df['label'].map({'pos': 1, 'neg': 0}).astype(int)\n","df.head()"],"metadata":{"id":"wjid1ORbwKI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into training and testing parts\n","x_train, x_test, y_train, y_test = train_test_split(df['review'], df['label'], test_size=0.2, random_state=42)"],"metadata":{"id":"fQL6L_BgzyH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set a tokenizer, and convert the numbers (labels) to sequences\n","# Example: \"This movie was great\" â†’ [23, 55, 12, 875]\n","\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocabSize, oov_token=\"<OOV>\")   # it is OOV, not zero-zero-V\n","tokenizer.fit_on_texts(x_train)\n","x_train_seq = tokenizer.texts_to_sequences(x_train)\n","x_test_seq = tokenizer.texts_to_sequences(x_test)"],"metadata":{"id":"AlzXRDID3Sci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Padding the sequences to limit the length of the review to 200 words only\n","\n","x_train_pad = tf.keras.preprocessing.sequence.pad_sequences(x_train_seq, maxlen=maxLen, padding='post')\n","x_test_pad = tf.keras.preprocessing.sequence.pad_sequences(x_test_seq, maxlen=maxLen, padding='post')"],"metadata":{"id":"L_losWNr4WDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Building the Neural Network Model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=vocabSize, output_dim=128, input_length=maxLen),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid'),\n","])"],"metadata":{"id":"l4OMVnh247xf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","history = model.fit(x_train_pad, y_train, batch_size=128, epochs=5, validation_data=(x_test_pad, y_test))"],"metadata":{"id":"rxsUsOS06ZkY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Accuracy vs Loss Plot\n","\n","pd.DataFrame(history.history).plot(figsize=(8, 6))\n","plt.title('Training and Validation Metrics')\n","plt.xlabel('Epochs')\n","plt.ylabel('Value')\n","plt.grid(True)  # optional\n","plt.show()"],"metadata":{"id":"6NiSRX_z8gJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluation of Test Data\n","\n","loss, accuracy = model.evaluate(x_test_pad, y_test)\n","print(f\"Test Accuracy: {100 * accuracy:.2f}%\")"],"metadata":{"id":"iaq7X-ex_ny1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make Predictions of each test review\n","\n","y_pred = model.predict(x_test_pad).flatten()\n","y_pred = (y_pred > 0.5).astype(int) # if prob > 0.5, class 1 (pos) else class 0 (negative)"],"metadata":{"id":"o3fqN5A6ARj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print Classification Report\n","print(metrics.classification_report(y_test, y_pred))"],"metadata":{"id":"aUBl8G84Bo0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confusion Matrix\n","cm = metrics.confusion_matrix(y_test, y_pred)\n","plot_confusion_matrix(cm, class_names=['Negative', 'Positive'])\n","plt.title('Confusion Matrix')\n","plt.show()"],"metadata":{"id":"v2ZSxosyB2Ch"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ðŸ”¹ **1. Imports & Setup**\n","\n","```python\n","import tensorflow as tf\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from mlxtend.plotting import plot_confusion_matrix\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline\n","```\n","\n","- **Importing libraries**:\n","  - `tensorflow`: For building and training our deep learning model.\n","  - `pandas`: For handling the dataset (reading CSV and working with columns).\n","  - `matplotlib`: To make plots (like accuracy/loss curves).\n","  - `sklearn`: For evaluation metrics like accuracy and confusion matrix.\n","  - `mlxtend`: To help visualize the confusion matrix clearly.\n","  - `warnings`: To hide any unnecessary warnings in output.\n","- `%matplotlib inline`: Only used in Jupyter Notebooks to show plots directly below code cells.\n","\n","---\n","\n","### ðŸ”¹ **2. Set Parameters**\n","\n","```python\n","vocab_size = 10000\n","max_len = 200\n","```\n","\n","- `vocab_size`: How many **unique words** we want the tokenizer to keep (top 10,000 most common words).\n","- `max_len`: The **maximum number of words per review**. If a review is shorter, we pad it; if it's longer, we cut it.\n","\n","---\n","\n","### ðŸ”¹ **3. Load and Prepare Data**\n","\n","```python\n","df = pd.read_csv('imdb_reviews.csv')  # Make sure this file has 'review' and 'sentiment' columns\n","\n","df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n","```\n","\n","- We load the dataset from a CSV file using pandas.\n","- The dataset should have two columns: `\"review\"` (text) and `\"sentiment\"` (positive/negative).\n","- We convert:\n","  - `\"positive\"` â†’ `1`\n","  - `\"negative\"` â†’ `0`  \n","  This is necessary because our model needs numbers to work, not words.\n","\n","---\n","\n","### ðŸ”¹ **4. Split the Dataset**\n","\n","```python\n","x_train, x_test, y_train, y_test = train_test_split(\n","    df['review'], df['sentiment'], test_size=0.2, random_state=42\n",")\n","```\n","\n","- We split our data:\n","  - `80%` for training the model (`x_train`, `y_train`)\n","  - `20%` for testing/evaluating it (`x_test`, `y_test`)\n","- `random_state=42` ensures we get the same split every time we run the code.\n","\n","---\n","\n","### ðŸ”¹ **5. Tokenization (Converting Words to Numbers)**\n","\n","```python\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(x_train)\n","```\n","\n","- We create a **tokenizer**, which converts text into sequences of numbers.\n","- It:\n","  - Keeps only the `vocab_size` most common words.\n","  - Uses `<OOV>` for words it doesn't recognize.\n","- `fit_on_texts`: It learns which words appear and assigns numbers to them.\n","\n","---\n","\n","### ðŸ”¹ **6. Convert Reviews to Number Sequences**\n","\n","```python\n","x_train_seq = tokenizer.texts_to_sequences(x_train)\n","x_test_seq = tokenizer.texts_to_sequences(x_test)\n","```\n","\n","- This converts each review into a list of numbers.\n","  - Example: `\"This movie was great\"` â†’ `[23, 55, 12, 875]`\n","\n","---\n","\n","### ðŸ”¹ **7. Padding the Sequences**\n","\n","```python\n","x_train_pad = tf.keras.preprocessing.sequence.pad_sequences(x_train_seq, maxlen=max_len, padding='post')\n","x_test_pad = tf.keras.preprocessing.sequence.pad_sequences(x_test_seq, maxlen=max_len, padding='post')\n","```\n","\n","- Ensures every review is the **same length** (`max_len = 200`).\n","- If a review is too short â†’ adds zeros at the end.\n","- If too long â†’ cuts extra words from the end.\n","\n","---\n","\n","### ðŸ”¹ **8. Build the Neural Network**\n","\n","```python\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","```\n","\n","This is our **text classification model**:\n","1. **Embedding Layer**:\n","   - Turns word numbers into word vectors.\n","   - Learns word meanings as it trains.\n","2. **Flatten Layer**:\n","   - Converts 2D embeddings into 1D so the next layer can process it.\n","3. **Dense Layer (Hidden Layer)**:\n","   - A fully connected layer with 128 neurons.\n","   - Uses ReLU activation to introduce non-linearity.\n","4. **Dense Layer (Output Layer)**:\n","   - Only 1 neuron (because it's binary classification).\n","   - Uses sigmoid to output a probability between 0 and 1.\n","\n","---\n","\n","### ðŸ”¹ **9. Compile the Model**\n","\n","```python\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","```\n","\n","- **Optimizer**: `'adam'` adjusts weights to reduce error.\n","- **Loss**: `'binary_crossentropy'` is good for binary classification.\n","- **Metric**: We measure performance using `'accuracy'`.\n","\n","---\n","\n","### ðŸ”¹ **10. Train the Model**\n","\n","```python\n","history = model.fit(x_train_pad, y_train, batch_size=128, epochs=5, validation_data=(x_test_pad, y_test))\n","```\n","\n","- Trains the model for 5 rounds (epochs) using the training data.\n","- It also checks performance on test data after each epoch (`validation_data`).\n","- Stores training info in `history`.\n","\n","---\n","\n","### ðŸ”¹ **11. Plot Training History**\n","\n","```python\n","pd.DataFrame(history.history).plot(figsize=(10,7))\n","plt.title(\"Training and Validation Metrics\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Value\")\n","plt.grid(True)\n","plt.show()\n","```\n","\n","- Plots how accuracy and loss change over the 5 epochs for both training and validation sets.\n","\n","---\n","\n","### ðŸ”¹ **12. Evaluate on Test Data**\n","\n","```python\n","loss, accuracy = model.evaluate(x_test_pad, y_test)\n","print(\"Test Accuracy:\", accuracy)\n","```\n","\n","- Checks how well the model performs on **unseen test data**.\n","\n","---\n","\n","### ðŸ”¹ **13. Make Predictions**\n","\n","```python\n","y_pred = model.predict(x_test_pad).flatten()\n","y_pred = (y_pred > 0.5).astype(int)\n","```\n","\n","- Predicts **probabilities** for each test review.\n","- Then converts them to labels:\n","  - If probability > 0.5 â†’ class 1 (positive)\n","  - Else â†’ class 0 (negative)\n","\n","---\n","\n","### ðŸ”¹ **14. Show Classification Report**\n","\n","```python\n","print(metrics.classification_report(y_test, y_pred))\n","```\n","- Shows precision, recall, and F1-score for both classes (positive and negative).\n","\n","---\n","\n","### ðŸ”¹ **15. Confusion Matrix**\n","\n","```python\n","cm = metrics.confusion_matrix(y_test, y_pred)\n","plot_confusion_matrix(cm, class_names=['Negative', 'Positive'])\n","plt.title(\"Confusion Matrix\")\n","plt.show()\n","```\n","\n","- Shows how many reviews were **correctly** or **incorrectly** classified.\n","- Helps spot if the model is better at one class than the other.\n","\n","---"],"metadata":{"id":"RCSU1TTZXgMb"}}]}